textbf@article{kg_朱,
  title={ナレッジグラフ構築のモデル化に関する考察},
  author={朱 成敏 and 小出 誠二 and 武田 英明 and 小柳 佑介 and 西野 文人},
  journal={人工知能学会第二種研究会資料},
  volume={2020},
  number={SWO-051},
  pages={01},
  year={2020},
  doi={10.11517/jsaisigtwo.2020.SWO-051_01}
}

@article{kg,
  title={第 1 回ナレッジグラフ推論チャレンジ 2018 開催報告 ―説明性のある人工知能システムを目指して―},
  author={川村隆浩 and 江上周作 and 田村光太郎 and 外園康智 and 鵜飼孝典 and 小柳佑介 and 西野文人 and 岡嶋成司 and 村上勝彦 and 高松邦彦 and 杉浦あおい and 白松俊 and 張翔宇 and 古崎晃司},
  journal={人工知能 34 巻 3 号},
  year={2019}
}

@inproceedings{TransE,
 author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Translating Embeddings for Modeling Multi-relational Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
 volume = {26},
 year = {2013}
}

@article{RotatE,
  author       = {Zhiqing Sun and
                  Zhi{-}Hong Deng and
                  Jian{-}Yun Nie and
                  Jian Tang},
  title        = {RotatE: Knowledge Graph Embedding by Relational Rotation in Complex
                  Space},
  journal      = {CoRR},
  volume       = {abs/1902.10197},
  year         = {2019},
  url          = {http://arxiv.org/abs/1902.10197},
  eprinttype    = {arXiv},
  eprint       = {1902.10197},
  timestamp    = {Fri, 26 Jul 2019 07:57:14 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1902-10197.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{ComplEx,
  title = 	 {Complex Embeddings for Simple Link Prediction},
  author = 	 {Trouillon, Théo and Welbl, Johannes and Riedel, Sebastian and Gaussier, Eric and Bouchard, Guillaume},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2071--2080},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/trouillon16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/trouillon16.html},
  abstract = 	 {In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.}
}

@inproceedings{AttH_etc,
    title = "Low-Dimensional Hyperbolic Knowledge Graph Embeddings",
    author = "Chami, Ines  and
      Wolf, Adva  and
      Juan, Da-Cheng  and
      Sala, Frederic  and
      Ravi, Sujith  and
      R{\'e}, Christopher",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.617",
    doi = "10.18653/v1/2020.acl-main.617",
    pages = "6901--6914",
    abstract = "Knowledge graph (KG) embeddings learn low- dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs. In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns. Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns. Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1{\%} in mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that different geometric transformations capture different types of relations while attention- based transformations generalize to multiple relations. In high dimensions, our approach yields new state-of-the-art MRRs of 49.6{\%} on WN18RR and 57.7{\%} on YAGO3-10.",
}

@article{MuRE,
  author       = {Ivana Balazevic and
                  Carl Allen and
                  Timothy M. Hospedales},
  title        = {Multi-relational Poincar{\'{e}} Graph Embeddings},
  journal      = {CoRR},
  volume       = {abs/1905.09791},
  year         = {2019},
  url          = {http://arxiv.org/abs/1905.09791},
  eprinttype    = {arXiv},
  eprint       = {1905.09791},
  timestamp    = {Wed, 29 May 2019 11:27:50 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1905-09791.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{CP,
      title={Canonical Tensor Decomposition for Knowledge Base Completion}, 
      author={Timothée Lacroix and Nicolas Usunier and Guillaume Obozinski},
      year={2018},
      eprint={1806.07297},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{kaneda,
  title={Story Knowledge Graph を用いた物語の内容推定モデルの提案と応用},
  author={金田燎弥},
  journal={修士論文},
  year={2023}
}

@article{BERT,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, MingWei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{KG-BERT,
  author       = {Liang Yao and
                  Chengsheng Mao and
                  Yuan Luo},
  title        = {{KG-BERT:} {BERT} for Knowledge Graph Completion},
  journal      = {CoRR},
  volume       = {abs/1909.03193},
  year         = {2019}, 
  url          = {http://arxiv.org/abs/1909.03193},
  eprinttype    = {arXiv},
  eprint       = {1909.03193}, 
  timestamp    = {Tue, 11 Jan 2022 16:58:16 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-03193.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{wn18rr,
  author       = {T Dettmers and P Minervini and P Stenetorp and S Riedel},
  title        = {Convolutional 2D Knowledge Graph Embeddings},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume       = {32(1)},
  year         = {2018},
}
