# このスクリプトは、prepare_data.pyで生成された中間JSONLファイルを読み込み、
# 各チャンクのテキストをベクトル化して、Supabaseのテーブルに一括で投入します。
# ベクトル化処理はGPUを利用して高速化されます。

import json
import os
import psycopg2
from openai import OpenAI
from tqdm import tqdm

# --- 設定項目 ---
INPUT_FILE = "prepared_anime_chunks.jsonl"
# SupabaseのIPv4用接続URI (必ず自分のものに書き換える！)
CONNECTION_STRING = "YOUR_VERY_STRONG"
# OpenAI APIキーを環境変数から読み込む
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
EMBEDDING_MODEL = "text-embedding-3-small"
TABLE_NAME = "anime_wiki_chunks" # 新しいテーブル名

def get_embedding_batch(texts, model=EMBEDDING_MODEL):
    """テキストのバッチからベクトルを取得する関数"""
    if not client.api_key or not texts:
        return [None] * len(texts)
    try:
        # 空白文字を置換
        texts = [text.replace("\n", " ") for text in texts]
        response = client.embeddings.create(input=texts, model=model)
        return [data.embedding for data in response.data]
    except Exception as e:
        print(f"OpenAI APIバッチ処理エラー: {e}")
        return [None] * len(texts)

def index_chunks_to_supabase():
    # データベースに接続
    conn = psycopg2.connect(CONNECTION_STRING)
    cursor = conn.cursor()

    # テーブルが存在しない場合は作成するSQL
    create_table_sql = f"""
    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
        id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
        anime_id INT NOT NULL,
        title_japanese TEXT,
        wiki_title TEXT,
        wiki_url TEXT,
        chunk_id INT NOT NULL,
        chunk_text TEXT,
        embedding vector(1536), -- OpenAI text-embedding-3-largeの次元数
        UNIQUE (anime_id, chunk_id) -- 同じアニメの同じチャンクが重複しないように
    );
    """
    cursor.execute(create_table_sql)
    conn.commit()
    print(f"テーブル '{TABLE_NAME}' の準備ができました。")

    # JSONLファイルからデータをバッチで読み込み、処理
    batch_size = 128 # 一度に処理するチャンクの数（APIの制限やメモリに応じて調整）
    batch = []
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        for line in tqdm(f, desc=f"Indexing chunks to {TABLE_NAME}"):
            batch.append(json.loads(line))
            
            if len(batch) >= batch_size:
                process_batch(cursor, batch)
                conn.commit() # バッチごとにコミット
                batch = []
        
        # 最後のバッチを処理
        if batch:
            process_batch(cursor, batch)
            conn.commit()

    cursor.close()
    conn.close()
    print("✅ 全てのチャンクのインデックス作成が完了しました。")

def process_batch(cursor, batch):
    """チャンクのバッチを処理し、DBに挿入する"""
    # テキストをベクトル化 (GPUがここで活きる)
    chunk_texts = [item['chunk_text'] for item in batch]
    embeddings = get_embedding_batch(chunk_texts)

    # INSERT文の準備
    insert_query = f"""
        INSERT INTO {TABLE_NAME} (
            anime_id, title_japanese, wiki_title, wiki_url,
            chunk_id, chunk_text, embedding
        ) VALUES (%s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (anime_id, chunk_id) DO NOTHING; -- 既に存在する場合は何もしない
    """
    
    # データをタプルのリストとしてまとめる
    data_to_insert = []
    for item, embedding in zip(batch, embeddings):
        if embedding:
            data_to_insert.append((
                item['anime_id'], item['title_japanese'],
                item['wiki_title'], item['wiki_url'],
                item['chunk_id'], item['chunk_text'],
                embedding
            ))
    
    # executemanyで一括挿入
    if data_to_insert:
        psycopg2.extras.execute_batch(cursor, insert_query, data_to_insert)

# psycopg2.extrasをインポートするのを忘れないように
import psycopg2.extras

if __name__ == "__main__":
    if not os.environ.get("OPENAI_API_KEY"):
        print("エラー: 環境変数 OPENAI_API_KEY が設定されていません。")
    else:
        index_chunks_to_supabase()